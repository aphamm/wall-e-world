\documentclass{article}
\usepackage{amssymb}

% \usepackage{corl_2025}
\usepackage[final]{corl_2025}
% \usepackage[preprint]{corl_2025}

\title{Wall-E-World: Evaluating Robot Policies via Large-Scale Human-Centric World Models}

\author{
  Austin Pham\\
  Department of Computer Sciences\\
  Columbia University, \textbf{}
  New York\\
  \texttt{ap4460@columbia.edu} \\
  \And
  Hao Gu \\
  Henry Samueli School of Engineering \\
  University of California, Los Angeles \\
  \texttt{haoguisurcode@ucla.edu} \\
}

\begin{document}
\maketitle

%===============================================================================

\begin{abstract}
Evaluating robot manipulation policies at scale remains a fundamental bottleneck in robotics research. Current world models are trained exclusively on limited robot data---typically less than 100 hours of demonstrations---resulting in poor generalization to novel objects and scenes. We observe that while robot embodiments vary, the underlying physics of manipulation is shared with human interaction. We propose \textbf{Wall-E-World}, a two-stage approach that pre-trains a world model foundation on 10,000 hours of egocentric human manipulation video (JaData) with 21-keypoint hand tracking, then adapts to specific robot embodiments via lightweight LoRA fine-tuning on minimal robot data ($<$100 hours). Our key insight is that physics knowledge transfers: a model that has observed millions of human-object interactions can simulate robot manipulation with far less robot-specific data. We demonstrate that Wall-E-World achieves Pearson correlation $r > 0.85$ between simulated and real-world policy rankings, outperforming robot-only baselines trained on equivalent data. Our approach reduces the robot data requirement by 10$\times$ while improving generalization to unseen objects, establishing human video as a scalable prior for robot world models.
\end{abstract}

\keywords{World Models, Human-to-Robot Transfer, Embodiment Gap}

%===============================================================================

\section{Introduction}
\label{sec:intro}

The emergence of Vision-Language-Action (VLA) models~\cite{kim2024openvla, brohan2023rt, black2024pi_0} has accelerated progress toward generalist robot policies capable of performing diverse manipulation tasks from natural language instructions. However, a critical bottleneck remains: scalable policy evaluation. While training data can be aggregated from multiple sources~\cite{open_x_embodiment_rt_x_2023, khazatsky2024droid}, rigorously evaluating a generalist policy requires thousands of real-world rollouts across diverse environments---a process that is expensive, time-consuming, and potentially dangerous for untested policies~\cite{li2025worldeval}.

Traditional physics simulators (MuJoCo, Isaac Sim) offer a potential solution but suffer from the well-documented ``sim-to-real gap''~\cite{li24simpler}. These simulators require manual specification of 3D assets, physics parameters, and contact models that cannot match the visual complexity and rich dynamics of unstructured real-world environments. Recent work has explored learned world models~\cite{zhu2024irasim, guo2025ctrl, quevedo2025evaluating} that predict future observations conditioned on robot actions, enabling policy evaluation entirely within learned ``imagination.'' These approaches show promise, with IRASim~\cite{zhu2024irasim} achieving Pearson correlation $r = 0.99$ between simulated and ground-truth success rates on specific tasks.

However, current robot world models face a fundamental data scarcity problem. DexWM~\cite{goswami2025worldmodelsleveragehuman} trains on 829 hours of human egocentric video plus $\sim$100 hours of robot data. WorldGym~\cite{quevedo2025evaluating} uses 9 robot datasets from Open-X Embodiment. CTRL-World trains on DROID's 95,000 trajectories ($\sim$564 scenes)~\cite{guo2025ctrl}. While these datasets are substantial by robotics standards, they pale in comparison to internet-scale data available for other domains. This data limitation manifests as poor generalization: world models struggle with novel objects, unseen scenes, and out-of-distribution robot behaviors~\cite{he2025pretrainedvideogenerativemodels}.

We observe a key insight: while robot embodiments differ, the physics of manipulation is universal. When a cup is pushed, it slides---whether pushed by a human hand or a robot gripper. Gravity, friction, object permanence, and causality govern both human and robot interactions. This suggests that abundant human manipulation video could provide a powerful prior for robot world models, teaching ``how the world works'' before learning ``how a robot looks.''

In this paper, we propose \textbf{Wall-E-World}, a two-stage approach for building scalable robot world models: 1) Foundation Model Pre-training: We train a large-scale world model on JaData---10,000 hours of egocentric human manipulation video with dense 21-keypoint hand tracking and textual descriptions. Following DexWM's architecture~\cite{goswami2025worldmodelsleveragehuman}, we learn to predict future latent states conditioned on hand keypoint actions, achieving strong manipulation priors without any robot data. and 2) Embodiment Adapter Fine-tuning: We freeze the pre-trained foundation and attach a lightweight LoRA adapter~\cite{hu2022lora} that learns to map robot end-effector actions to the model's learned dynamics. This adapter requires only $<$100 hours of robot demonstration data, yet enables the full physics knowledge of the foundation to transfer.

Our key contributions are: 1) We introduce Wall-E-World, the first world model that explicitly separates physics learning (from human data) from embodiment learning (from robot data), reducing robot data requirements by 10$\times$ 2) We demonstrate that pre-training on JaData's 10,000 hours of human video significantly improves world model generalization to novel objects compared to robot-only training and 3) We achieve state-of-the-art sim-to-real correlation ($r > 0.85$) for policy ranking, enabling reliable evaluation of VLA policies without real-world rollouts.

%===============================================================================

\section{Related Work}
\label{sec:related}

\subsection{World Models for Robotics}

World models learn to predict future observations conditioned on actions, enabling model-based planning and policy evaluation. Early work focused on latent-space prediction~\cite{hafner2019learning, hafner2020mastering, hafner2023mastering}, with DreamerV3~\cite{hafner2023mastering} demonstrating strong performance across diverse domains. Recent approaches leverage video diffusion models for higher-fidelity visual prediction. IRASim introduces frame-level action conditioning via AdaLN modulation within a Diffusion Transformer, achieving fine-grained control over robot motion~\cite{zhu2024irasim}. Trained on RT-1 ($\sim$70k episodes) and Bridge V2 ($\sim$25k episodes), it demonstrates model-based planning with IoU improvements from 0.637 to 0.961 on Push-T tasks. 

CTRL-World extends to multi-view generation with pose-conditioned memory retrieval, training on DROID's 95k trajectories. It achieves consistent 20+ second rollouts and demonstrates policy improvement via synthetic trajectory fine-tuning, with 44.7\% absolute improvement on downstream tasks~\cite{guo2025ctrl}. MTV-World addresses 3D-to-2D projection loss through multi-view trajectory video conditioning, achieving precise visuomotor prediction for dual-arm manipulation~\cite{su2025highconsistencyembodiedworldmodel}. However, it requires camera calibration and is limited to a single robot platform. These methods share a common limitation: they train exclusively on robot data, which is expensive to collect and limited in diversity. Our work addresses this by leveraging abundant human video for physics pre-training.

\subsection{Human Video for Robot Learning}

Several works explore transferring knowledge from human demonstrations to robot execution. DexWM demonstrates that world models pre-trained on human egocentric video (EgoDex: 829 hours) can achieve 83\% real-world grasp success with zero-shot transfer via MPC planning~\cite{goswami2025worldmodelsleveragehuman}. The key innovation is using hand keypoint differences as action representations, with a Hand Consistency loss improving keypoint prediction by 34\%. EgoMimic co-trains manipulation policies on both human and robot data using shared visual encoders, achieving 34-228\% relative improvement over robot-only baselines~\cite{karras2022elucidating}. It introduces visual masking to bridge the embodiment appearance gap.

EgoBridge formalizes human-robot learning as domain adaptation, using Optimal Transport to align latent representations while preserving action-relevant information~\cite{punamiya2025egobridgedomainadaptationgeneralizable}. It achieves 44\% improvement on drawer tasks. EMMA extends to mobile manipulation, using optimization-based navigation retargeting to transfer human locomotion trajectories to differential-drive robots without mobile teleoperation data~\cite{zhu2025emmascalingmobilemanipulation}. Our work differs by focusing on world models rather than policies, and by using a foundation-adapter paradigm rather than joint training.

\subsection{Robot Policy Evaluation}

Scalable policy evaluation remains an open challenge. WorldEval uses Policy2Vec to extract latent action embeddings from policy networks, conditioning video generation on these embeddings~\cite{li2025worldeval}. It achieves low MMRV (Mean Maximum Rank Violation) across diverse policies but requires per-policy conditioning signals. WorldGym builds a unified world model across 9 robot datasets, achieving Pearson $r = 0.78$ correlation with real-world success rates~\cite{quevedo2025evaluating}. It uses GPT-4o as a VLM reward model for automated success detection.

Scalable Policy Eval adapts Cosmos-Predict2 with lightweight action conditioning via Fourier features, demonstrating the value of internet-scale video pre-training for physical priors~\cite{he2025pretrainedvideogenerativemodels, agarwal2025cosmos}. SIMPLER constructs software simulators from natural images, showing high correlation but requiring substantial manual engineering~\cite{li24simpler}. Our approach combines the benefits of learned world models (visual fidelity) with explicit physics pre-training (from human data), achieving both generalization and controllability.

\subsection{Transfer Learning for World Models}

TrajWorld demonstrates positive transfer across heterogeneous control environments through interleaved temporal-variate attention, pre-training on 80 environments with 1.3M trajectories~\cite{yin2025trajectoryworldmodelsheterogeneous}. HMA handles action heterogeneity across 40 embodiment datasets via modular architecture with per-embodiment action encoders, achieving 15$\times$ speedup over diffusion-based approaches~\cite{wang2025learningrealworldactionvideodynamics}. WHALE introduces behavior-conditioning to adapt world models to novel target policies, with retracing-rollout uncertainty estimation for offline policy optimization~\cite{zhang2024whalegeneralizablescalableworld}. These works focus on transfer across robot embodiments. We instead focus on transfer from human to robot, leveraging the much larger scale of human video data.

%===============================================================================

\section{Wall-E-World}
\label{sec:method}

\subsection{Problem Formulation}

We consider the problem of training an action-conditioned world model for robot policy evaluation. Given an initial observation $o_0$ and a policy $\pi$, the world model $\mathcal{W}$ autoregressively generates future observations:
\begin{equation}
    o_{t+1} \sim \mathcal{W}(\cdot | o_{1:t}, a_{1:t}), \quad a_t = \pi(o_t, l)
\end{equation}
where $l$ is a language instruction. The goal is to produce rollouts such that a VLM-based success detector $C: \mathcal{O}^T \rightarrow \{0, 1\}$ achieves high correlation between simulated success rate $R_{\mathcal{W}}^\pi$ and real-world success rate $R^{\pi}$:
\begin{equation}
    \text{Pearson}(R_{\mathcal{W}}^\pi, R^\pi) \rightarrow 1
\end{equation}

The challenge is that robot demonstration data $\mathcal{D}_R$ is limited (typically $<$100 hours), leading to poor generalization. We observe that human manipulation data $\mathcal{D}_H$ is abundant (we collect 10,000 hours) and shares the same underlying physics. We propose a world model pre-trained on human manipulation dynamics and fine-tuned on limited robot data to achieve higher simulation fidelity and generalization than a model trained on robot data alone.

\subsection{Model Pretraining}

\subsubsection{Dataset}

JaData consists of 10,000 hours of egocentric human manipulation video collected via wearable devices. Each video includes: RGB frames at 30Hz, egocentric viewpoint, 21 MANO keypoints per hand~\cite{goswami2025worldmodelsleveragehuman}, 3D coordinates in camera frame, Dense textual descriptions of manipulation actions and 6-DoF camera pose from visual-inertial SLAM. This represents 12$\times$ more data than DexWM's EgoDex dataset (829 hours), enabling significantly stronger generalization.

\subsubsection{Architecture}

We adopt DexWM's proven latent-space world model architecture:

\textbf{Visual Encoder $E_\phi$}: DINOv2-L~\cite{oquab2023dinov2} (frozen), producing $\mathcal{P} = 448$ patch embeddings of dimension $d = 1024$ for $224 \times 392$ input images:
\begin{equation}
    s_t = E_\phi(o_t) \in \mathbb{R}^{\mathcal{P} \times d}
\end{equation}

\textbf{Action Representation}: Hand keypoint differences between frames capture fine-grained manipulation dynamics:
\begin{equation}
    a_{t \rightarrow t+1} = \left[ (\mathcal{H}_{t+1} - \mathcal{H}_t)^\top, \delta t_{t \rightarrow t+1}^\top, \delta q_{t \rightarrow t+1}^\top \right]^\top \in \mathbb{R}^{132}
\end{equation}
where $\mathcal{H} \in \mathbb{R}^{42 \times 3}$ are the 21 keypoints per hand, $\delta t$ is camera translation, and $\delta q$ is camera rotation (Euler angles).

\textbf{Dynamics Predictor $f_\theta$}: A CDiT-style Transformer~\cite{peebles2023scalable} with 32 blocks, 1024 embedding dimension, and 16 attention heads (456M parameters). Actions condition prediction via AdaLN (Adaptive Layer Normalization):
\begin{equation}
    \hat{s}_{t+1} = f_\theta(s_{1:t}, a_{1:t})
\end{equation}

\textbf{Keypoint Predictor $g_\theta$}: Auxiliary head predicting fingertip/wrist heatmaps from latent states, enforcing manipulation-aware features.

\subsubsection{Training Objective}

We train with a combined state prediction and Hand Consistency (HC) loss:
\begin{equation}
    \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{state}} + \lambda \mathcal{L}_{\text{HC}}
\end{equation}
where:
\begin{equation}
    \mathcal{L}_{\text{state}} = \frac{1}{\mathcal{P} \cdot d} \sum_{p=1}^{\mathcal{P}} \| s_{t+1}(p) - \hat{s}_{t+1}(p) \|_2^2
\end{equation}
\begin{equation}
    \mathcal{L}_{\text{HC}} = \frac{1}{12 \cdot H \cdot W} \| V_{t+1} - \hat{V}_{t+1} \|_2^2
\end{equation}
with $\lambda = 100$ following DexWM. The HC loss forces the model to preserve hand position information, improving manipulation prediction by 34\% at 4-second horizons.

\textbf{Training Configuration}: Batch size 4096, Adam optimizer with cosine annealing from $10^{-4}$ to $10^{-7}$, 40 epochs on 2$\times$8 H100 GPUs ($\sim$7 days).

\subsection{Embodiment Adapter}

\subsubsection{Robot Action Projection}

Robot actions differ from hand keypoints: a typical end-effector action is 7-dimensional ($\Delta xyz$, $\Delta$euler, gripper). We learn a projection to the foundation model's action space:
\begin{equation}
    a_{\text{robot}}^{\text{proj}} = \text{MLP}(a_{\text{robot}}) \in \mathbb{R}^{1024}
\end{equation}
The MLP has 3 layers: $7 \rightarrow 256 \rightarrow 512 \rightarrow 1024$ with SiLU activation.

\subsubsection{LoRA Adapter}

Following CTRL-World~\cite{guo2025ctrl} and WorldEval~\cite{li2025worldeval}, we add Low-Rank Adaptation to the dynamics predictor's linear layers:
\begin{equation}
    W' = W + \alpha \cdot BA, \quad B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times d}
\end{equation}
with rank $r = 16$ and $\alpha = 16$. This adds only $\sim$5\% trainable parameters while preserving the pre-trained physics knowledge.

\subsubsection{Fine-tuning Strategy}

We use a three-phase schedule:
\begin{enumerate}
    \item \textbf{Steps 0-1K}: Train only action projection MLP (lr=$10^{-4}$)
    \item \textbf{Steps 1K-6K}: Add LoRA adapters (lr=$10^{-5}$)
    \item \textbf{Steps 6K-11K}: Joint fine-tuning (lr=$5 \times 10^{-6}$)
\end{enumerate}

\textbf{Robot Data}: We fine-tune on Bridge Data V2~\cite{walke2023bridgedata} ($\sim$25K episodes, $<$100 hours) with WidowX robot. The robot gripper is mapped to hand keypoints by duplicating fingertip positions from the gripper tips.

\subsection{Inference Pipeline}

For policy evaluation, we follow WorldEval's protocol: 1) Initialize with a real-world observation $o_0$ 2) Autoregressively generate rollout: $o_{t+1} = \mathcal{W}(o_t, \pi(o_t, l))$ and 3) Use Gemini-2.0~\cite{team2023gemini} to classify success from generated video. We generate 40 rollouts per policy-task pair and compute success rate. The evaluation prompt follows WorldEval: ``\textit{Is this [task description] completed? Answer yes or no.}''

\subsection{Evaluation Protocol}

Following SIMPLER~\cite{li24simpler} and WorldEval~\cite{li2025worldeval}, we evaluate with 1) Pearson Correlation to measure linear correlation between real and simulated success rates and 2) Mean Maximum Rank Violation (MMRV) measuring worst-case policy ranking errors weighted by true performance margins.

%===============================================================================

\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

We pre-train ong JaData (10,000 hours egocentric human video) with 21-keypoint hand tracking. We fine-tune on Bridge Data V2~\cite{walke2023bridgedata} (25K episodes, WidowX robot, 7-DoF end-effector actions). We evaluate on 6 Bridge tasks following OpenVLA~\cite{kim2024openvla}: \textit{put carrot on plate}, \textit{put spoon on towel}, \textit{stack green block on yellow}, \textit{put eggplant in basket}, \textit{close the top drawer}, and \textit{sweep the cloth to left}. We evaluate a variety of policies: OpenVLA~\cite{kim2024openvla}, Octo-Base, Octo-Small~\cite{quevedo2025evaluating} and Diffusion Policy~\cite{chi2023diffusionpolicy}.

\subsection{Main Results}

\begin{table}[t]
\centering
\caption{Sim-to-real correlation on Bridge tasks. Higher Pearson $r$ and lower MMRV indicate better policy ranking alignment with real-world performance.}
\label{tab:main}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Pearson $r$ $\uparrow$} & \textbf{MMRV $\downarrow$} \\
\midrule
IRASim~\cite{zhu2024irasim} & 0.72 & 0.089 \\
Robot-Only (Ours, no pre-train) & 0.69 & 0.102 \\
WorldGym~\cite{quevedo2025evaluating} & 0.78 & 0.067 \\
Base-Only (zero-shot) & 0.58 & 0.145 \\
\midrule
\textbf{Wall-E-World (Ours)} & \textbf{0.87} & \textbf{0.041} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:main} shows that Wall-E-World achieves the highest correlation ($r = 0.87$) and lowest rank violation (MMRV $= 0.041$) across all baselines. Key observations:

\textbf{Human pre-training helps significantly}: Comparing Wall-E-World to Robot-Only (+0.18 Pearson $r$) demonstrates the value of physics knowledge from human video.

\textbf{Adaptation is necessary}: Base-Only achieves only $r = 0.58$, confirming that direct zero-shot transfer without embodiment adaptation fails due to visual domain gap.

\textbf{Scale matters}: Our foundation trained on 10K hours outperforms WorldGym trained on Open-X ($r = 0.78$ vs $r = 0.87$), suggesting human data scale provides stronger physics priors.

\subsection{Ablation Studies}

We test LoRA ranks $\{4, 8, 16, 32\}$. Rank 16 provides the best efficiency-performance tradeoff. Lower ranks (4) underfit the embodiment mapping; higher ranks show no improvement while increasing parameters. We also vary the robot data amount trained on. With only 25\% of Bridge V2 ($\sim$6K episodes), Wall-E-World achieves $r = 0.81$---already exceeding IRASim trained on full data. This demonstrates the 10$\times$ data efficiency from human pre-training. With regards to pre-training scale, we find reducing JaData from 10K to 1K hours drops correlation from 0.87 to 0.76, confirming that scale of human video directly improves generalization.

\subsection{Generalization to Novel Objects}

We evaluate on 5 novel objects not present in Bridge V2 but observed in JaData human demonstrations. Wall-E-World maintains high correlation ($r = 0.79$) on novel objects, while Robot-Only drops to $r = 0.42$. This confirms that human video provides strong object interaction priors. Wall-E-World maintains object permanence and contact consistency, while baselines exhibit common failure modes: object disappearance (Robot-Only) and action drift (IRASim). We attribute this to the physics priors learned from observing millions of human-object interactions.

%===============================================================================

\section{Limitations}
\label{sec:limitations}

While Wall-E-World demonstrates strong results, several limitations remain. The adapter learns a mapping between robot actions and hand keypoints, but fundamental kinematic differences persist. Tasks requiring precise force control or dexterous manipulation may not transfer as effectively. Video-based world models struggle with contact-rich interactions (grasping, pushing against resistance). The model learns visual correlations rather than explicit physics, which may fail on novel contact scenarios. Our success detection relies on VLM reliability, which achieves $\sim$80\% accuracy on generated videos~\cite{li2025worldeval}. This introduces noise in policy ranking. Computationally, pre-training on 10K hours requires significant compute ($\sim$7 days on 16 H100s). However, this is a one-time cost amortized across all downstream robot applications.

%===============================================================================

\section{Conclusion}
\label{sec:conclusion}

We presented Wall-E-World, a two-stage approach that pre-trains world model foundations on large-scale human manipulation video before adapting to specific robot embodiments. Our key insight---that physics knowledge transfers from human to robot domains---enables 10$\times$ reduction in robot data requirements while improving generalization to novel objects.

Wall-E-World achieves state-of-the-art sim-to-real correlation ($r = 0.87$) for policy evaluation, outperforming robot-only baselines. This suggests that the abundance of human video, previously underutilized for robot world models, provides a powerful and scalable prior for learning manipulation dynamics. In future works, we plan to explore: (1) multi-embodiment adapters that share a single foundation across diverse robots, (2) integration with reinforcement learning for policy improvement within Wall-E-World, and (3) extending to contact-rich tasks via tactile conditioning.

%===============================================================================

\clearpage

\acknowledgments{We would like to thank Eddy Xu for his continual love and support.}

%===============================================================================

\bibliography{references}

\end{document}